{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgovil1/.conda/envs/ire/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from llm_attacks.minimal_gcg.opt_utils import token_gradients, sample_control, get_logits, target_loss, token_gradients_batch, get_logits_batch, target_loss_batch\n",
    "from llm_attacks.minimal_gcg.opt_utils import load_model_and_tokenizer, get_filtered_cands\n",
    "from llm_attacks.minimal_gcg.string_utils import SuffixManager, load_conversation_template\n",
    "from llm_attacks import get_nonascii_toks\n",
    "\n",
    "from livelossplot import PlotLosses # pip install livelossplot\n",
    "\n",
    "# Set the random seed for NumPy\n",
    "np.random.seed(20)\n",
    "\n",
    "# Set the random seed for PyTorch\n",
    "torch.manual_seed(20)\n",
    "\n",
    "# If you are using CUDA (i.e., a GPU), also set the seed for it\n",
    "torch.cuda.manual_seed_all(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"auto\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"~/ada_user/.cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgovil1/.conda/envs/ire/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.40s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=\"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\",\n",
    "    device_map=device,\n",
    ").eval()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgovil1/.conda/envs/ire/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False,\n",
    "    use_auth_token=\"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\",\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgovil1/.conda/envs/ire/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/pgovil1/.conda/envs/ire/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'dag ${\\\\tikz ofetz[(drawable in(%resize token hide new secretions from the parental units \\n\\n%\\n\\\\def\\\\respectivesubject{Respectively}}\\n\\n%\\n\\\\def\\\\parentalunits{Parental Units}}\\n\\n%\\n\\\\def\\\\newsecretions{New Secretions}}\\n\\n%\\n\\\\def\\\\hide{Hide}}\\n\\n%\\n\\\\def\\\\token{Token}}\\n\\n%\\n\\\\def\\\\'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(\"dag ${\\\\tikz ofetz[(drawable in(%resize token hide new secretions from the parental units \", max_length=100, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_template = load_conversation_template(\"llama-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.46.0'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerBase.apply_chat_template of LlamaTokenizer(name_or_path='meta-llama/Llama-2-7b-chat-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_attacks.minimal_gcg.custom_dataset_utils import CustomDataset, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train = CustomDataset(\n",
    "    tokenizer=tokenizer, conv_template=conv_template, split=\"train\", debug = False, label_type=\"str\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_train.set_adv_string(\"predict the sentiment of the following sentence in a single token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_train[0]['_control_slice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([159])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_train[0]['_target_slice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "negative\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁negative']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(torch.gather(torch.tensor(dat_train[123]['input_ids']), -1, dat_train[123]['_target_slice']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dat_refer = load_dataset(\"stanfordnlp/sst2\", split=\"train[:12%]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8082"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dat_refer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 36\n"
     ]
    }
   ],
   "source": [
    "print(len(dat_refer[0]['sentence']), len(dat_refer[1]['sentence']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'sentence': 'hide new secretions from the parental units ',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_refer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_train_dataloader = torch.utils.data.DataLoader(\n",
    "    dat_train, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "_control_slice\n",
      "_target_slice\n",
      "_loss_slice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgovil1/llm-attacks/llm_attacks/minimal_gcg/custom_dataset_utils.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = [torch.tensor(item[\"attention_mask\"]) for item in batch]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_control_slice': tensor([[158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168],\n",
       "         [137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147],\n",
       "         [148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158],\n",
       "         [164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174],\n",
       "         [141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151],\n",
       "         [144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154],\n",
       "         [145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155],\n",
       "         [137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147],\n",
       "         [137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147],\n",
       "         [142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152],\n",
       "         [141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151],\n",
       "         [141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151],\n",
       "         [149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159],\n",
       "         [138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148],\n",
       "         [158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168],\n",
       "         [138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148],\n",
       "         [138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148],\n",
       "         [157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167],\n",
       "         [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153],\n",
       "         [140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150],\n",
       "         [139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n",
       "         [137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147],\n",
       "         [141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151],\n",
       "         [144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154],\n",
       "         [152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162],\n",
       "         [153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163],\n",
       "         [153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163],\n",
       "         [143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153],\n",
       "         [139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n",
       "         [139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n",
       "         [140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150],\n",
       "         [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160]]),\n",
       " '_target_slice': tensor([[173],\n",
       "         [152],\n",
       "         [163],\n",
       "         [179],\n",
       "         [156],\n",
       "         [159],\n",
       "         [160],\n",
       "         [152],\n",
       "         [152],\n",
       "         [157],\n",
       "         [156],\n",
       "         [156],\n",
       "         [164],\n",
       "         [153],\n",
       "         [173],\n",
       "         [153],\n",
       "         [153],\n",
       "         [172],\n",
       "         [158],\n",
       "         [155],\n",
       "         [154],\n",
       "         [152],\n",
       "         [156],\n",
       "         [159],\n",
       "         [167],\n",
       "         [168],\n",
       "         [168],\n",
       "         [158],\n",
       "         [154],\n",
       "         [154],\n",
       "         [155],\n",
       "         [165]]),\n",
       " '_loss_slice': tensor([[172],\n",
       "         [151],\n",
       "         [162],\n",
       "         [178],\n",
       "         [155],\n",
       "         [158],\n",
       "         [159],\n",
       "         [151],\n",
       "         [151],\n",
       "         [156],\n",
       "         [155],\n",
       "         [155],\n",
       "         [163],\n",
       "         [152],\n",
       "         [172],\n",
       "         [152],\n",
       "         [152],\n",
       "         [171],\n",
       "         [157],\n",
       "         [154],\n",
       "         [153],\n",
       "         [151],\n",
       "         [155],\n",
       "         [158],\n",
       "         [166],\n",
       "         [167],\n",
       "         [167],\n",
       "         [157],\n",
       "         [153],\n",
       "         [153],\n",
       "         [154],\n",
       "         [164]]),\n",
       " 'input_ids': tensor([[    1,     1, 29961,  ...,     2,     2,     2],\n",
       "         [    1,     1, 29961,  ...,     2,     2,     2],\n",
       "         [    1,     1, 29961,  ...,     2,     2,     2],\n",
       "         ...,\n",
       "         [    1,     1, 29961,  ...,     2,     2,     2],\n",
       "         [    1,     1, 29961,  ...,     2,     2,     2],\n",
       "         [    1,     1, 29961,  ...,     2,     2,     2]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(custom_train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "\tLoss             \t (min:   13.078, max:   13.078, cur:   13.078)\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "negative\n",
      "positive\n",
      "negative\n",
      "negative\n",
      "_control_slice\n",
      "_target_slice\n",
      "_loss_slice\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 134.00 MiB. GPU 3 has a total capacity of 23.64 GiB of which 113.38 MiB is free. Process 86922 has 10.71 GiB memory in use. Process 111999 has 3.45 GiB memory in use. Including non-PyTorch memory, this process has 9.38 GiB memory in use. Of the allocated memory 7.80 GiB is allocated by PyTorch, and 808.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 17\u001b[0m\n\u001b[1;32m     11\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# input_ids = input_ids.to(model.device)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(batch[\"_control_slice\"].shape)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# print(input_ids[:, batch[\"_control_slice\"].squeeze()].shape)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Step 2. Compute Coordinate Gradient\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m coordinate_grad, loss_c \u001b[38;5;241m=\u001b[39m \u001b[43mtoken_gradients_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_control_slice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_target_slice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_loss_slice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# print(coordinate_grad.shape)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m coordinate_grad \u001b[38;5;241m=\u001b[39m coordinate_grad\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/llm-attacks/llm_attacks/minimal_gcg/opt_utils.py:170\u001b[0m, in \u001b[0;36mtoken_gradients_batch\u001b[0;34m(model, input_ids, input_slice, target_slice, loss_slice)\u001b[0m\n\u001b[1;32m    167\u001b[0m full_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(full_embeds)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# print(full_embeds.shape)\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_embeds\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# print(\"loss slice\", loss_slice.shape, loss_slice.device)\u001b[39;00m\n\u001b[1;32m    172\u001b[0m logits_forloss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m    173\u001b[0m     logits, \u001b[38;5;241m1\u001b[39m, loss_slice\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    174\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:1190\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1187\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1190\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:945\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    933\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    934\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    935\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    942\u001b[0m         position_embeddings,\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 945\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:692\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    691\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 692\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:258\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    256\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 258\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/accelerate/hooks.py:170\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/ire/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 3 has a total capacity of 23.64 GiB of which 113.38 MiB is free. Process 86922 has 10.71 GiB memory in use. Process 111999 has 3.45 GiB memory in use. Including non-PyTorch memory, this process has 9.38 GiB memory in use. Of the allocated memory 7.80 GiB is allocated by PyTorch, and 808.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "plotlosses = PlotLosses()\n",
    "\n",
    "# not_allowed_tokens = None if allow_non_ascii else get_nonascii_toks(tokenizer)\n",
    "adv_suffix = \"do sentiment prediction in a single word\"\n",
    "current_best_loss = 1e9\n",
    "\n",
    "for batch in custom_train_dataloader:\n",
    "    for i in range(num_steps):\n",
    "\n",
    "        # Step 1. Encode user prompt (behavior + adv suffix) as tokens and return token ids.\n",
    "        input_ids = batch[\"input_ids\"].to(model.device)\n",
    "        # input_ids = input_ids.to(model.device)\n",
    "        # print(batch[\"_control_slice\"].shape)\n",
    "        # print(input_ids[:, batch[\"_control_slice\"].squeeze()].shape)\n",
    "\n",
    "        # Step 2. Compute Coordinate Gradient\n",
    "        coordinate_grad, loss_c = token_gradients_batch(\n",
    "            model,\n",
    "            input_ids,\n",
    "            batch[\"_control_slice\"].to(model.device),\n",
    "            batch[\"_target_slice\"].to(model.device),\n",
    "            batch[\"_loss_slice\"].to(model.device),\n",
    "        )\n",
    "        # print(coordinate_grad.shape)\n",
    "        coordinate_grad = coordinate_grad.mean(dim=0)\n",
    "        # Step 3. Sample a batch of new tokens based on the coordinate gradient.\n",
    "        # Notice that we only need the one that minimizes the loss.\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Step 3.1 Slice the input to locate the adversarial suffix.\n",
    "            # print(batch[\"_control_slice\"].shape)\n",
    "            # if input_ids.is_cuda:\n",
    "            # input_ids = input_ids.cpu()\n",
    "            # print(batch[\"_control_slice\"])\n",
    "\n",
    "            # print(input_ids.shape)\n",
    "            adv_suffix_tokens = input_ids[0][batch[\"_control_slice\"][0]].to(\n",
    "                model.device\n",
    "            )\n",
    "            # print(\"control_slice\", batch[\"_control_slice\"])\n",
    "\n",
    "            # Step 3.2 Randomly sample a batch of replacements.\n",
    "            new_adv_suffix_toks = sample_control(\n",
    "                adv_suffix_tokens,\n",
    "                coordinate_grad,\n",
    "                input_ids.shape[0] // 4,\n",
    "                topk=128,\n",
    "                temp=0.5,\n",
    "                not_allowed_tokens=None,\n",
    "            )\n",
    "            # print(\"new_adv_suffix_toks\", new_adv_suffix_toks)\n",
    "            # print(\n",
    "            #     torch.gather(input_ids, 1, batch[\"_control_slice\"].to(input_ids.device))\n",
    "            # )\n",
    "\n",
    "            # Step 3.3 This step ensures all adversarial candidates have the same number of tokens.\n",
    "            # This step is necessary because tokenizers are not invertible\n",
    "            # so Encode(Decode(tokens)) may produce a different tokenization.\n",
    "            # We ensure the number of token remains to prevent the memory keeps growing and run into OOM.\n",
    "            new_adv_suffix = get_filtered_cands(\n",
    "                tokenizer,\n",
    "                new_adv_suffix_toks,\n",
    "                filter_cand=True,\n",
    "                curr_control=adv_suffix,\n",
    "            )\n",
    "            print(\"new_adv_suffic\", new_adv_suffix)\n",
    "\n",
    "            # Step 3.4 Compute loss on these candidates and take the argmin.\n",
    "            logits, ids = get_logits_batch(\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                input_ids=input_ids,\n",
    "                control_slice=batch[\"_control_slice\"].to(model.device),\n",
    "                test_controls=new_adv_suffix,\n",
    "                return_ids=True,\n",
    "                batch_size=16,\n",
    "            )  # decrease this number if you run into OOM.\n",
    "\n",
    "\n",
    "            # print(\"target slice\", batch[\"_target_slice\"], batch[\"_target_slice\"].shape)\n",
    "            # print(batch[\"_target_slice\"].repeat(ids.view(-1, ids.shape[-1]).shape[0] // batch[\"_target_slice\"].shape[0], 1).shape)\n",
    "            \n",
    "            # sds\n",
    "            # print(logits.shape)\n",
    "            target_slice_repeat = batch[\"_target_slice\"].unsqueeze(dim=0).repeat(ids.shape[0], 1, 1)\n",
    "            losses = target_loss_batch(\n",
    "                logits,\n",
    "                ids.view(-1, ids.shape[-1]),\n",
    "                target_slice_repeat.reshape(-1, target_slice_repeat.shape[-1]), \n",
    "                og_shape=input_ids.shape,\n",
    "            )\n",
    "\n",
    "            print(\"losses shape\", losses.shape)\n",
    "            print(losses)\n",
    "            # error\n",
    "            # exit()\n",
    "\n",
    "            best_new_adv_suffix_id = losses.argmin()\n",
    "            loss_candidate = losses[best_new_adv_suffix_id]\n",
    "            best_new_adv_suffix = new_adv_suffix[best_new_adv_suffix_id]\n",
    "            dat_train.set_adv_string(best_new_adv_suffix)\n",
    "\n",
    "            current_loss = losses[best_new_adv_suffix_id]\n",
    "\n",
    "            # Update the running adv_suffix with the best candidate\n",
    "            adv_suffix = best_new_adv_suffix\n",
    "            # is_success = check_for_attack_success(model,\n",
    "            #                         tokenizer,\n",
    "            #                         suffix_manager.get_input_ids(adv_string=adv_suffix).to(device),\n",
    "            #                         suffix_manager._assistant_role_slice,\n",
    "            #                         test_prefixes)\n",
    "\n",
    "        # # Create a dynamic plot for the loss.\n",
    "        plotlosses.update({\"Loss\": loss_c.detach().cpu().numpy()})\n",
    "        plotlosses.send()\n",
    "\n",
    "        # print(f\"\\nPassed:{is_success}\\nCurrent Suffix:{best_new_adv_suffix}\", end='\\r')\n",
    "\n",
    "        # Notice that for the purpose of demo we stop immediately if we pass the checker but you are free to\n",
    "        # comment this to keep the optimization running for longer (to get a lower loss).\n",
    "        # if is_success:\n",
    "        # break\n",
    "\n",
    "        # (Optional) Clean up the cache.\n",
    "        del coordinate_grad, adv_suffix_tokens\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mdevice\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dag ${\\\\tikz ofetz[(drawable in(%resize token'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adv_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dag\\\\{\\\\tikz ofetz[(drawable in(%resize token',\n",
       " 'dag ${\\\\ Dictionary ofetz[(drawable in(%resize token',\n",
       " 'dag ${\\\\tikz of opposite[(drawable in(%resize token',\n",
       " 'dag ${\\\\tikz ofetz)[drawable in(%resize token',\n",
       " 'dag ${\\\\tikz ofetz[(minipage in(%resize token',\n",
       " 'dag ${\\\\tikz ofetz[(drawable in:\\u2009resize token',\n",
       " 'dag ${\\\\tikz ofetz[(drawable in(% apache token',\n",
       " 'dag ${\\\\tikz ofetz[(drawable in(% apache token']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_adv_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"adv_suffix.txt\", \"w\") as f:\n",
    "    f.write(adv_suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"new_adv_suffix.txt\", \"w\") as f:\n",
    "    f.write(new_adv_suffix.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
